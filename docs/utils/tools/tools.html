<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>pydbsmgr.utils.tools.tools API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pydbsmgr.utils.tools.tools</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import concurrent.futures
import glob
import os
import random
import re
import sys
from collections import Counter
from typing import Callable, List, Tuple

import numpy as np
import pandas as pd
import psutil
import pyarrow as pa
import pyarrow.parquet as pq
import yaml
from numpy import datetime64
from pandas.core.frame import DataFrame
from pandas.errors import IntCastingNaNError
from pyarrow import Table

from pydbsmgr.main import check_if_contains_dates, is_number_regex
from pydbsmgr.utils.config import load_config, parse_config


def disableprints(func: Callable) -&gt; Callable:
    &#34;&#34;&#34;Decorator to temporarily suppress print statements in a function&#34;&#34;&#34;

    def wrapper(*args, **kwargs):
        sys.stdout = open(os.devnull, &#34;w&#34;)
        result = func(*args, **kwargs)
        sys.stdout = sys.__stdout__
        return result

    # Preserve the function&#39;s docstring if it has one
    if func.__doc__ is not None:
        wrapper.__doc__ = func.__doc__
        return wrapper
    else:
        return func


def most_repeated_item(items: list, two_most_common: bool = False) -&gt; Tuple[str, str | None]:
    &#34;&#34;&#34;Returns a `Tuple` with the most common elements of a `list`.

    Parameters
    ----------
    items : `list`
        the `list` containing the items to be evaluated.
    two_most_common : `bool`, `optional`
        If `False`, returns only one element. Defaults to `False`.

    Returns
    -------
    Tuple[`str`, `str` | `None`]
        The two most common elements.
    &#34;&#34;&#34;
    # Use Counter to count occurrences of each item in the list
    counter = Counter(items)

    # Find the two most common items and its count
    most_common = counter.most_common(2)

    if two_most_common:
        if len(most_common) == 2:
            item1, _ = most_common[0]
            item2, _ = most_common[1]
            return item1, item2
        else:
            item, _ = most_common[0]
            return item, None
    else:
        item, _ = most_common[0]
        return item, None


def generate_secure_password(pass_len: int = 24) -&gt; str:
    &#34;&#34;&#34;
    Generate a secure password with the length specified
    &#34;&#34;&#34;
    config = load_config(&#34;./pydbsmgr/utils/config.ini&#34;)
    config = parse_config(config)
    password = &#34;&#34;
    char_matrix = config[&#34;security&#34;][&#34;char_matrix&#34;]
    for _ in range(pass_len):
        password = password + random.choice(char_matrix)

    return password


class ColumnsCheck:
    &#34;&#34;&#34;Performs the relevant checks on the columns of the `DataFrame`&#34;&#34;&#34;

    def __init__(self, df: DataFrame):
        self.df = df

    def get_frame(self, **kwargs) -&gt; DataFrame:
        self.df = self._process_columns(**kwargs)
        return self.df

    def _process_columns(self, surrounding: bool = True) -&gt; DataFrame:
        df = (self.df).copy()
        df.columns = df.columns.str.lower()
        df.columns = df.columns.str.replace(&#34;.&#34;, &#34;&#34;, regex=False)
        df.columns = df.columns.str.replace(&#34;,&#34;, &#34;&#34;, regex=False)
        df.columns = df.columns.str.replace(r&#34;[^a-zA-Z0-9ñáéíóú_]&#34;, &#34;_&#34;, regex=True)

        df.columns = df.columns.str.replace(&#34;_+&#34;, &#34;_&#34;, regex=True)
        df.columns = df.columns.str.strip()
        df.columns = df.columns.str.strip(&#34;_&#34;)
        if surrounding:
            df.columns = [f&#34;[{col}]&#34; for col in df.columns]

        return df


def coerce_datetime(x: str) -&gt; datetime64:
    try:
        x = x.replace(&#34;-&#34;, &#34;&#34;)
        return pd.to_datetime(x, format=&#34;%Y%m%d&#34;)
    except:
        return np.datetime64(&#34;NaT&#34;)


class ControllerFeatures:
    def __init__(self, _container_client):
        self._container_client = _container_client

    def write_pyarrow(
        self,
        directory_name: str,
        pytables: List[Table],
        names: List[str],
        overwrite: bool = True,
    ) -&gt; None:
        &#34;&#34;&#34;Write pyarrow table as `parquet` format&#34;&#34;&#34;
        format_type = &#34;parquet&#34;
        files_not_loaded = []
        for table, blob_name in zip(pytables, names):
            if table != None:
                buf = pa.BufferOutputStream()
                pq.write_table(table, buf)
                parquet_data = buf.getvalue().to_pybytes()
                blob_path_name = directory_name + &#34;/&#34; + blob_name
                self._container_client.upload_blob(
                    name=blob_path_name + &#34;.&#34; + format_type,
                    data=parquet_data,
                    overwrite=overwrite,
                )
            else:
                files_not_loaded.append(blob_name)
        if len(files_not_loaded) &gt; 0:
            return files_not_loaded

    def write_parquet(
        self,
        directory_name: str,
        dfs: List[DataFrame],
        names: List[str],
        overwrite: bool = True,
        upload: bool = True,
    ) -&gt; None:
        &#34;&#34;&#34;Write dataframes as `parquet` format by converting them first into `bytes`.&#34;&#34;&#34;
        files = []
        format_type = &#34;parquet&#34;
        files_not_loaded = []
        for data, blob_name in zip(dfs, names):
            if data != None:
                table = pa.Table.from_pandas(data)
                buf = pa.BufferOutputStream()
                pq.write_table(table, buf)
                parquet_data = buf.getvalue().to_pybytes()
                blob_path_name = directory_name + &#34;/&#34; + blob_name
                if upload:
                    self._container_client.upload_blob(
                        name=blob_path_name + &#34;.&#34; + format_type,
                        data=parquet_data,
                        overwrite=overwrite,
                    )
                else:
                    files.append(parquet_data)
            else:
                files_not_loaded.append(blob_name)
        if len(files_not_loaded) &gt; 0:
            return files_not_loaded

        if not upload:
            return files


def column_coincidence(df1: DataFrame, df2: DataFrame) -&gt; float:
    &#34;&#34;&#34;Return the percentage of coincident columns between two pandas dataframes.&#34;&#34;&#34;
    if not isinstance(df1, pd.DataFrame) or not isinstance(df2, pd.DataFrame):
        raise ValueError(&#34;Both inputs should be pandas DataFrames&#34;)

    column_names1 = set(df1.columns)
    column_names2 = set(df2.columns)

    common_columns = column_names1.intersection(column_names2)
    total_columns = column_names1.union(column_names2)

    coincidence_percentage = len(common_columns) / len(total_columns)
    return coincidence_percentage


def merge_by_coincidence(df1: DataFrame, df2: DataFrame, tol: float = 0.9) -&gt; DataFrame:
    &#34;&#34;&#34;Merge two pandas dataframes by finding the most similar columns based on their names.&#34;&#34;&#34;
    percentage = column_coincidence(df1, df2)
    total_columns = set(df1.columns).union(set(df2.columns))
    num_col1 = len(df1.columns)
    num_col2 = len(df2.columns)
    if num_col1 &lt; num_col2:
        min_cols = set(df1.columns)
        min_cols = list(min_cols.intersection(set(df2.columns)))
    else:
        min_cols = set(df2.columns)
        min_cols = list(min_cols.intersection(set(df1.columns)))

    df2 = (df2[min_cols]).copy()
    df1 = (df1[min_cols]).copy()
    diff = total_columns.difference(set(min_cols))

    df = pd.concat([df1, df2], ignore_index=True)
    if percentage &gt; tol:
        print(&#34;The following columns were lost : &#34;, diff)
    else:
        print(
            f&#34;The following columns were missed with a match percentage of {percentage*100:.2f}% : &#34;,
            diff,
        )
    return df


def terminate_process_holding_file(file_path):
    for proc in psutil.process_iter([&#34;pid&#34;, &#34;open_files&#34;]):
        try:
            if any(file_path in file_info.path for file_info in proc.open_files()):
                print(f&#34;Terminating process {proc.pid} holding the file.&#34;)
                proc.terminate()
        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):
            pass


def erase_files(format: str = &#34;log&#34;) -&gt; None:
    for filename in glob.glob(&#34;*.&#34; + format):
        try:
            os.remove(filename)
        except:
            terminate_process_holding_file(filename)
            os.remove(filename)


def get_extraction_date(
    filename: str | List[str], REGEX_PATTERN: str = r&#34;\d{4}-\d{2}-\d{2}&#34;
) -&gt; str:
    &#34;&#34;&#34;Allows to extract the date of extraction according to the directory within the storage account.

    Parameters
    ----------
    filename : `str` | List[`str`]
        file path inside the storage account
    REGEX_PATTERN : `str`, `optional`
        regular expression pattern to extract the date. Defaults to `r&#34;\d{4}-\d{2}-\d{2}&#34;`.

    Returns
    -------
    `str`
        the date that was extracted if found in the file path.
    &#34;&#34;&#34;

    def sub_extraction_date(filename: str, REGEX_PATTERN: str) -&gt; str:
        extraction_date = re.findall(REGEX_PATTERN, filename)
        if len(extraction_date) &gt; 0:
            _date = extraction_date[0]
        else:
            _date = &#34;&#34;
        return _date

    if type(filename) == str:
        return sub_extraction_date(filename, REGEX_PATTERN)
    elif isinstance(filename, list):
        dates = []
        for name in filename:
            dates.append(sub_extraction_date(name, REGEX_PATTERN))
        return dates


class ColumnsDtypes:
    &#34;&#34;&#34;Convert all columns to specified dtype.&#34;&#34;&#34;

    def __init__(self, df_: DataFrame):
        self.df = df_.copy()

    def correct(
        self,
        drop_values: bool = False,
        drop_rows: bool = False,
        sample_frac: float = 0.1,
    ) -&gt; DataFrame:
        self._check_int_float(drop_values, drop_rows)
        self._check_datetime(sample_frac)
        return self.df

    def get_frame(self) -&gt; DataFrame:
        return self.df

    def _check_int_float(self, drop_values: bool = False, drop_rows: bool = False) -&gt; None:
        &#34;&#34;&#34;Check and correct the data types of columns in a `DataFrame`.&#34;&#34;&#34;

        def check_float(x):
            if isinstance(x, str):
                try:
                    return float(x)
                except:
                    return np.nan
            else:
                return x

        def check_int(x):
            if isinstance(x, str):
                try:
                    return int(x)
                except:
                    return &#34;&#34;
            else:
                return x

        df_ = (self.df).copy()
        if drop_values:
            if len(df_) &lt; 1e5:
                for col in df_.columns:
                    value = str(df_[col].iloc[0])
                    val_dtype = None
                    if is_number_regex(value):
                        if type(check_float(value)).__name__ == &#34;float&#34;:
                            with concurrent.futures.ThreadPoolExecutor() as executor:
                                df_[col] = list(executor.map(check_float, df_[col]))
                            df_[col] = df_[col].astype(&#34;float64&#34;)
                            val_dtype = &#34;float64&#34;
                            print(&#34;Checking {%s} for column {%s}&#34; % (val_dtype, col))

                        if type(check_int(value)).__name__ == &#34;int&#34; and val_dtype == None:
                            with concurrent.futures.ThreadPoolExecutor() as executor:
                                df_[col] = list(executor.map(check_int, df_[col]))
                            if drop_rows:
                                df_ = df_.loc[df_[col].notnull()]
                            try:
                                df_[col] = df_[col].astype(&#34;int64&#34;)
                                val_dtype = &#34;int64&#34;
                                print(&#34;Checking {%s} for column {%s}&#34; % (val_dtype, col))
                            except IntCastingNaNError as e:
                                df_[col] = df_[col].astype(&#34;object&#34;)
                                val_dtype = &#34;object&#34;
                                print(&#34;Checking {%s} for column {%s}&#34; % (val_dtype, col))

                        print(f&#34;Successfully transformed the &#39;{col}&#39; column into {val_dtype}.&#34;)
        self.df = df_

    def _check_datetime(self, sample_frac: float) -&gt; None:
        &#34;&#34;&#34;Check and convert date-time string columns to `datetime` objects.&#34;&#34;&#34;
        df_ = self.df
        cols = df_.columns
        df_sample = df_.sample(frac=sample_frac)
        for column_index, datatype in enumerate(df_.dtypes):
            col = cols[column_index]
            if datatype == &#34;object&#34;:
                datetype_column = (df_sample[col].apply(check_if_contains_dates)).isin([True]).any()
                if datetype_column:
                    try:
                        with concurrent.futures.ThreadPoolExecutor() as executor:
                            df_[col] = list(
                                executor.map(lambda date: date.replace(&#34;-&#34;, &#34;&#34;), df_[col])
                            )
                        df_[col] = pd.to_datetime(df_[col], format=&#34;%Y%m%d&#34;).dt.normalize()
                        print(f&#34;Successfully transformed the &#39;{col}&#39; column into datetime64[ns].&#34;)
                    except:
                        with concurrent.futures.ThreadPoolExecutor() as executor:
                            df_[col] = list(executor.map(coerce_datetime, df_[col]))
                        df_[col] = pd.to_datetime(df_[col], format=&#34;%Y%m%d&#34;, errors=&#34;coerce&#34;)
                        print(f&#34;Successfully transformed the &#39;{col}&#39; column into datetime64[ns].&#34;)
            elif datatype == &#34;datetime64[us]&#34; or datatype == &#34;datetime64[ns]&#34;:
                df_[col] = df_[col].astype(&#34;datetime64[ns]&#34;)
                df_[col] = df_[col].dt.normalize()
                print(f&#34;Successfully transformed the &#39;{col}&#39; column into datetime64[ns].&#34;)

        self.df = df_


def create_directory(data, parent_path=&#34;&#34;):
    &#34;&#34;&#34;Creates the directory tree from a `yaml` file.&#34;&#34;&#34;
    for key, value in data.items():
        path = os.path.join(parent_path, key)
        if isinstance(value, dict):
            os.makedirs(path, exist_ok=True)
            create_directory(value, path)
        else:
            os.makedirs(path, exist_ok=True)


def create_directories_from_yaml(yaml_file):
    &#34;&#34;&#34;Reads a `yaml` file and creates directories based on its content.&#34;&#34;&#34;
    with open(yaml_file, &#34;r&#34;) as file:
        data = yaml.safe_load(file)
        create_directory(data)


if __name__ == &#34;__main__&#34;:
    yaml_file = &#34;directories.yaml&#34;
    create_directories_from_yaml(yaml_file)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="pydbsmgr.utils.tools.tools.coerce_datetime"><code class="name flex">
<span>def <span class="ident">coerce_datetime</span></span>(<span>x: str) ‑> numpy.datetime64</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def coerce_datetime(x: str) -&gt; datetime64:
    try:
        x = x.replace(&#34;-&#34;, &#34;&#34;)
        return pd.to_datetime(x, format=&#34;%Y%m%d&#34;)
    except:
        return np.datetime64(&#34;NaT&#34;)</code></pre>
</details>
</dd>
<dt id="pydbsmgr.utils.tools.tools.column_coincidence"><code class="name flex">
<span>def <span class="ident">column_coincidence</span></span>(<span>df1: pandas.core.frame.DataFrame, df2: pandas.core.frame.DataFrame) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>Return the percentage of coincident columns between two pandas dataframes.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def column_coincidence(df1: DataFrame, df2: DataFrame) -&gt; float:
    &#34;&#34;&#34;Return the percentage of coincident columns between two pandas dataframes.&#34;&#34;&#34;
    if not isinstance(df1, pd.DataFrame) or not isinstance(df2, pd.DataFrame):
        raise ValueError(&#34;Both inputs should be pandas DataFrames&#34;)

    column_names1 = set(df1.columns)
    column_names2 = set(df2.columns)

    common_columns = column_names1.intersection(column_names2)
    total_columns = column_names1.union(column_names2)

    coincidence_percentage = len(common_columns) / len(total_columns)
    return coincidence_percentage</code></pre>
</details>
</dd>
<dt id="pydbsmgr.utils.tools.tools.create_directories_from_yaml"><code class="name flex">
<span>def <span class="ident">create_directories_from_yaml</span></span>(<span>yaml_file)</span>
</code></dt>
<dd>
<div class="desc"><p>Reads a <code>yaml</code> file and creates directories based on its content.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_directories_from_yaml(yaml_file):
    &#34;&#34;&#34;Reads a `yaml` file and creates directories based on its content.&#34;&#34;&#34;
    with open(yaml_file, &#34;r&#34;) as file:
        data = yaml.safe_load(file)
        create_directory(data)</code></pre>
</details>
</dd>
<dt id="pydbsmgr.utils.tools.tools.create_directory"><code class="name flex">
<span>def <span class="ident">create_directory</span></span>(<span>data, parent_path='')</span>
</code></dt>
<dd>
<div class="desc"><p>Creates the directory tree from a <code>yaml</code> file.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_directory(data, parent_path=&#34;&#34;):
    &#34;&#34;&#34;Creates the directory tree from a `yaml` file.&#34;&#34;&#34;
    for key, value in data.items():
        path = os.path.join(parent_path, key)
        if isinstance(value, dict):
            os.makedirs(path, exist_ok=True)
            create_directory(value, path)
        else:
            os.makedirs(path, exist_ok=True)</code></pre>
</details>
</dd>
<dt id="pydbsmgr.utils.tools.tools.disableprints"><code class="name flex">
<span>def <span class="ident">disableprints</span></span>(<span>func: Callable) ‑> Callable</span>
</code></dt>
<dd>
<div class="desc"><p>Decorator to temporarily suppress print statements in a function</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def disableprints(func: Callable) -&gt; Callable:
    &#34;&#34;&#34;Decorator to temporarily suppress print statements in a function&#34;&#34;&#34;

    def wrapper(*args, **kwargs):
        sys.stdout = open(os.devnull, &#34;w&#34;)
        result = func(*args, **kwargs)
        sys.stdout = sys.__stdout__
        return result

    # Preserve the function&#39;s docstring if it has one
    if func.__doc__ is not None:
        wrapper.__doc__ = func.__doc__
        return wrapper
    else:
        return func</code></pre>
</details>
</dd>
<dt id="pydbsmgr.utils.tools.tools.erase_files"><code class="name flex">
<span>def <span class="ident">erase_files</span></span>(<span>format: str = 'log') ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def erase_files(format: str = &#34;log&#34;) -&gt; None:
    for filename in glob.glob(&#34;*.&#34; + format):
        try:
            os.remove(filename)
        except:
            terminate_process_holding_file(filename)
            os.remove(filename)</code></pre>
</details>
</dd>
<dt id="pydbsmgr.utils.tools.tools.generate_secure_password"><code class="name flex">
<span>def <span class="ident">generate_secure_password</span></span>(<span>pass_len: int = 24) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Generate a secure password with the length specified</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_secure_password(pass_len: int = 24) -&gt; str:
    &#34;&#34;&#34;
    Generate a secure password with the length specified
    &#34;&#34;&#34;
    config = load_config(&#34;./pydbsmgr/utils/config.ini&#34;)
    config = parse_config(config)
    password = &#34;&#34;
    char_matrix = config[&#34;security&#34;][&#34;char_matrix&#34;]
    for _ in range(pass_len):
        password = password + random.choice(char_matrix)

    return password</code></pre>
</details>
</dd>
<dt id="pydbsmgr.utils.tools.tools.get_extraction_date"><code class="name flex">
<span>def <span class="ident">get_extraction_date</span></span>(<span>filename: Union[str, List[str]], REGEX_PATTERN: str = '\\d{4}-\\d{2}-\\d{2}') ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Allows to extract the date of extraction according to the directory within the storage account.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code><code>str&lt;/code&gt; | List[</code>str<code>]</code></dt>
<dd>file path inside the storage account</dd>
<dt><strong><code>REGEX_PATTERN</code></strong> :&ensp;<code>str<code>,</code>optional</code></dt>
<dd>regular expression pattern to extract the date. Defaults to <code>r"\d{4}-\d{2}-\d{2}"</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>str</code>
the date that was extracted if found in the file path.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_extraction_date(
    filename: str | List[str], REGEX_PATTERN: str = r&#34;\d{4}-\d{2}-\d{2}&#34;
) -&gt; str:
    &#34;&#34;&#34;Allows to extract the date of extraction according to the directory within the storage account.

    Parameters
    ----------
    filename : `str` | List[`str`]
        file path inside the storage account
    REGEX_PATTERN : `str`, `optional`
        regular expression pattern to extract the date. Defaults to `r&#34;\d{4}-\d{2}-\d{2}&#34;`.

    Returns
    -------
    `str`
        the date that was extracted if found in the file path.
    &#34;&#34;&#34;

    def sub_extraction_date(filename: str, REGEX_PATTERN: str) -&gt; str:
        extraction_date = re.findall(REGEX_PATTERN, filename)
        if len(extraction_date) &gt; 0:
            _date = extraction_date[0]
        else:
            _date = &#34;&#34;
        return _date

    if type(filename) == str:
        return sub_extraction_date(filename, REGEX_PATTERN)
    elif isinstance(filename, list):
        dates = []
        for name in filename:
            dates.append(sub_extraction_date(name, REGEX_PATTERN))
        return dates</code></pre>
</details>
</dd>
<dt id="pydbsmgr.utils.tools.tools.merge_by_coincidence"><code class="name flex">
<span>def <span class="ident">merge_by_coincidence</span></span>(<span>df1: pandas.core.frame.DataFrame, df2: pandas.core.frame.DataFrame, tol: float = 0.9) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Merge two pandas dataframes by finding the most similar columns based on their names.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge_by_coincidence(df1: DataFrame, df2: DataFrame, tol: float = 0.9) -&gt; DataFrame:
    &#34;&#34;&#34;Merge two pandas dataframes by finding the most similar columns based on their names.&#34;&#34;&#34;
    percentage = column_coincidence(df1, df2)
    total_columns = set(df1.columns).union(set(df2.columns))
    num_col1 = len(df1.columns)
    num_col2 = len(df2.columns)
    if num_col1 &lt; num_col2:
        min_cols = set(df1.columns)
        min_cols = list(min_cols.intersection(set(df2.columns)))
    else:
        min_cols = set(df2.columns)
        min_cols = list(min_cols.intersection(set(df1.columns)))

    df2 = (df2[min_cols]).copy()
    df1 = (df1[min_cols]).copy()
    diff = total_columns.difference(set(min_cols))

    df = pd.concat([df1, df2], ignore_index=True)
    if percentage &gt; tol:
        print(&#34;The following columns were lost : &#34;, diff)
    else:
        print(
            f&#34;The following columns were missed with a match percentage of {percentage*100:.2f}% : &#34;,
            diff,
        )
    return df</code></pre>
</details>
</dd>
<dt id="pydbsmgr.utils.tools.tools.most_repeated_item"><code class="name flex">
<span>def <span class="ident">most_repeated_item</span></span>(<span>items: list, two_most_common: bool = False) ‑> Tuple[str, str | None]</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a <code>Tuple</code> with the most common elements of a <code>list</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>items</code></strong> :&ensp;<code>list</code></dt>
<dd>the <code>list</code> containing the items to be evaluated.</dd>
<dt><strong><code>two_most_common</code></strong> :&ensp;<code>bool<code>,</code>optional</code></dt>
<dd>If <code>False</code>, returns only one element. Defaults to <code>False</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Tuple[<code>str</code>, <code>str</code> | <code>None</code>]
The two most common elements.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def most_repeated_item(items: list, two_most_common: bool = False) -&gt; Tuple[str, str | None]:
    &#34;&#34;&#34;Returns a `Tuple` with the most common elements of a `list`.

    Parameters
    ----------
    items : `list`
        the `list` containing the items to be evaluated.
    two_most_common : `bool`, `optional`
        If `False`, returns only one element. Defaults to `False`.

    Returns
    -------
    Tuple[`str`, `str` | `None`]
        The two most common elements.
    &#34;&#34;&#34;
    # Use Counter to count occurrences of each item in the list
    counter = Counter(items)

    # Find the two most common items and its count
    most_common = counter.most_common(2)

    if two_most_common:
        if len(most_common) == 2:
            item1, _ = most_common[0]
            item2, _ = most_common[1]
            return item1, item2
        else:
            item, _ = most_common[0]
            return item, None
    else:
        item, _ = most_common[0]
        return item, None</code></pre>
</details>
</dd>
<dt id="pydbsmgr.utils.tools.tools.terminate_process_holding_file"><code class="name flex">
<span>def <span class="ident">terminate_process_holding_file</span></span>(<span>file_path)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def terminate_process_holding_file(file_path):
    for proc in psutil.process_iter([&#34;pid&#34;, &#34;open_files&#34;]):
        try:
            if any(file_path in file_info.path for file_info in proc.open_files()):
                print(f&#34;Terminating process {proc.pid} holding the file.&#34;)
                proc.terminate()
        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):
            pass</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="pydbsmgr.utils.tools.tools.ColumnsCheck"><code class="flex name class">
<span>class <span class="ident">ColumnsCheck</span></span>
<span>(</span><span>df: pandas.core.frame.DataFrame)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs the relevant checks on the columns of the <code>DataFrame</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ColumnsCheck:
    &#34;&#34;&#34;Performs the relevant checks on the columns of the `DataFrame`&#34;&#34;&#34;

    def __init__(self, df: DataFrame):
        self.df = df

    def get_frame(self, **kwargs) -&gt; DataFrame:
        self.df = self._process_columns(**kwargs)
        return self.df

    def _process_columns(self, surrounding: bool = True) -&gt; DataFrame:
        df = (self.df).copy()
        df.columns = df.columns.str.lower()
        df.columns = df.columns.str.replace(&#34;.&#34;, &#34;&#34;, regex=False)
        df.columns = df.columns.str.replace(&#34;,&#34;, &#34;&#34;, regex=False)
        df.columns = df.columns.str.replace(r&#34;[^a-zA-Z0-9ñáéíóú_]&#34;, &#34;_&#34;, regex=True)

        df.columns = df.columns.str.replace(&#34;_+&#34;, &#34;_&#34;, regex=True)
        df.columns = df.columns.str.strip()
        df.columns = df.columns.str.strip(&#34;_&#34;)
        if surrounding:
            df.columns = [f&#34;[{col}]&#34; for col in df.columns]

        return df</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="pydbsmgr.fast_upload.DataFrameToSQL" href="../../fast_upload.html#pydbsmgr.fast_upload.DataFrameToSQL">DataFrameToSQL</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="pydbsmgr.utils.tools.tools.ColumnsCheck.get_frame"><code class="name flex">
<span>def <span class="ident">get_frame</span></span>(<span>self, **kwargs) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_frame(self, **kwargs) -&gt; DataFrame:
    self.df = self._process_columns(**kwargs)
    return self.df</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pydbsmgr.utils.tools.tools.ColumnsDtypes"><code class="flex name class">
<span>class <span class="ident">ColumnsDtypes</span></span>
<span>(</span><span>df_: pandas.core.frame.DataFrame)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert all columns to specified dtype.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ColumnsDtypes:
    &#34;&#34;&#34;Convert all columns to specified dtype.&#34;&#34;&#34;

    def __init__(self, df_: DataFrame):
        self.df = df_.copy()

    def correct(
        self,
        drop_values: bool = False,
        drop_rows: bool = False,
        sample_frac: float = 0.1,
    ) -&gt; DataFrame:
        self._check_int_float(drop_values, drop_rows)
        self._check_datetime(sample_frac)
        return self.df

    def get_frame(self) -&gt; DataFrame:
        return self.df

    def _check_int_float(self, drop_values: bool = False, drop_rows: bool = False) -&gt; None:
        &#34;&#34;&#34;Check and correct the data types of columns in a `DataFrame`.&#34;&#34;&#34;

        def check_float(x):
            if isinstance(x, str):
                try:
                    return float(x)
                except:
                    return np.nan
            else:
                return x

        def check_int(x):
            if isinstance(x, str):
                try:
                    return int(x)
                except:
                    return &#34;&#34;
            else:
                return x

        df_ = (self.df).copy()
        if drop_values:
            if len(df_) &lt; 1e5:
                for col in df_.columns:
                    value = str(df_[col].iloc[0])
                    val_dtype = None
                    if is_number_regex(value):
                        if type(check_float(value)).__name__ == &#34;float&#34;:
                            with concurrent.futures.ThreadPoolExecutor() as executor:
                                df_[col] = list(executor.map(check_float, df_[col]))
                            df_[col] = df_[col].astype(&#34;float64&#34;)
                            val_dtype = &#34;float64&#34;
                            print(&#34;Checking {%s} for column {%s}&#34; % (val_dtype, col))

                        if type(check_int(value)).__name__ == &#34;int&#34; and val_dtype == None:
                            with concurrent.futures.ThreadPoolExecutor() as executor:
                                df_[col] = list(executor.map(check_int, df_[col]))
                            if drop_rows:
                                df_ = df_.loc[df_[col].notnull()]
                            try:
                                df_[col] = df_[col].astype(&#34;int64&#34;)
                                val_dtype = &#34;int64&#34;
                                print(&#34;Checking {%s} for column {%s}&#34; % (val_dtype, col))
                            except IntCastingNaNError as e:
                                df_[col] = df_[col].astype(&#34;object&#34;)
                                val_dtype = &#34;object&#34;
                                print(&#34;Checking {%s} for column {%s}&#34; % (val_dtype, col))

                        print(f&#34;Successfully transformed the &#39;{col}&#39; column into {val_dtype}.&#34;)
        self.df = df_

    def _check_datetime(self, sample_frac: float) -&gt; None:
        &#34;&#34;&#34;Check and convert date-time string columns to `datetime` objects.&#34;&#34;&#34;
        df_ = self.df
        cols = df_.columns
        df_sample = df_.sample(frac=sample_frac)
        for column_index, datatype in enumerate(df_.dtypes):
            col = cols[column_index]
            if datatype == &#34;object&#34;:
                datetype_column = (df_sample[col].apply(check_if_contains_dates)).isin([True]).any()
                if datetype_column:
                    try:
                        with concurrent.futures.ThreadPoolExecutor() as executor:
                            df_[col] = list(
                                executor.map(lambda date: date.replace(&#34;-&#34;, &#34;&#34;), df_[col])
                            )
                        df_[col] = pd.to_datetime(df_[col], format=&#34;%Y%m%d&#34;).dt.normalize()
                        print(f&#34;Successfully transformed the &#39;{col}&#39; column into datetime64[ns].&#34;)
                    except:
                        with concurrent.futures.ThreadPoolExecutor() as executor:
                            df_[col] = list(executor.map(coerce_datetime, df_[col]))
                        df_[col] = pd.to_datetime(df_[col], format=&#34;%Y%m%d&#34;, errors=&#34;coerce&#34;)
                        print(f&#34;Successfully transformed the &#39;{col}&#39; column into datetime64[ns].&#34;)
            elif datatype == &#34;datetime64[us]&#34; or datatype == &#34;datetime64[ns]&#34;:
                df_[col] = df_[col].astype(&#34;datetime64[ns]&#34;)
                df_[col] = df_[col].dt.normalize()
                print(f&#34;Successfully transformed the &#39;{col}&#39; column into datetime64[ns].&#34;)

        self.df = df_</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="pydbsmgr.utils.tools.tools.ColumnsDtypes.correct"><code class="name flex">
<span>def <span class="ident">correct</span></span>(<span>self, drop_values: bool = False, drop_rows: bool = False, sample_frac: float = 0.1) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def correct(
    self,
    drop_values: bool = False,
    drop_rows: bool = False,
    sample_frac: float = 0.1,
) -&gt; DataFrame:
    self._check_int_float(drop_values, drop_rows)
    self._check_datetime(sample_frac)
    return self.df</code></pre>
</details>
</dd>
<dt id="pydbsmgr.utils.tools.tools.ColumnsDtypes.get_frame"><code class="name flex">
<span>def <span class="ident">get_frame</span></span>(<span>self) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_frame(self) -&gt; DataFrame:
    return self.df</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pydbsmgr.utils.tools.tools.ControllerFeatures"><code class="flex name class">
<span>class <span class="ident">ControllerFeatures</span></span>
<span>(</span><span>_container_client)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ControllerFeatures:
    def __init__(self, _container_client):
        self._container_client = _container_client

    def write_pyarrow(
        self,
        directory_name: str,
        pytables: List[Table],
        names: List[str],
        overwrite: bool = True,
    ) -&gt; None:
        &#34;&#34;&#34;Write pyarrow table as `parquet` format&#34;&#34;&#34;
        format_type = &#34;parquet&#34;
        files_not_loaded = []
        for table, blob_name in zip(pytables, names):
            if table != None:
                buf = pa.BufferOutputStream()
                pq.write_table(table, buf)
                parquet_data = buf.getvalue().to_pybytes()
                blob_path_name = directory_name + &#34;/&#34; + blob_name
                self._container_client.upload_blob(
                    name=blob_path_name + &#34;.&#34; + format_type,
                    data=parquet_data,
                    overwrite=overwrite,
                )
            else:
                files_not_loaded.append(blob_name)
        if len(files_not_loaded) &gt; 0:
            return files_not_loaded

    def write_parquet(
        self,
        directory_name: str,
        dfs: List[DataFrame],
        names: List[str],
        overwrite: bool = True,
        upload: bool = True,
    ) -&gt; None:
        &#34;&#34;&#34;Write dataframes as `parquet` format by converting them first into `bytes`.&#34;&#34;&#34;
        files = []
        format_type = &#34;parquet&#34;
        files_not_loaded = []
        for data, blob_name in zip(dfs, names):
            if data != None:
                table = pa.Table.from_pandas(data)
                buf = pa.BufferOutputStream()
                pq.write_table(table, buf)
                parquet_data = buf.getvalue().to_pybytes()
                blob_path_name = directory_name + &#34;/&#34; + blob_name
                if upload:
                    self._container_client.upload_blob(
                        name=blob_path_name + &#34;.&#34; + format_type,
                        data=parquet_data,
                        overwrite=overwrite,
                    )
                else:
                    files.append(parquet_data)
            else:
                files_not_loaded.append(blob_name)
        if len(files_not_loaded) &gt; 0:
            return files_not_loaded

        if not upload:
            return files</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="pydbsmgr.utils.azure_sdk.StorageController" href="../azure_sdk.html#pydbsmgr.utils.azure_sdk.StorageController">StorageController</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="pydbsmgr.utils.tools.tools.ControllerFeatures.write_parquet"><code class="name flex">
<span>def <span class="ident">write_parquet</span></span>(<span>self, directory_name: str, dfs: List[pandas.core.frame.DataFrame], names: List[str], overwrite: bool = True, upload: bool = True) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Write dataframes as <code>parquet</code> format by converting them first into <code>bytes</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write_parquet(
    self,
    directory_name: str,
    dfs: List[DataFrame],
    names: List[str],
    overwrite: bool = True,
    upload: bool = True,
) -&gt; None:
    &#34;&#34;&#34;Write dataframes as `parquet` format by converting them first into `bytes`.&#34;&#34;&#34;
    files = []
    format_type = &#34;parquet&#34;
    files_not_loaded = []
    for data, blob_name in zip(dfs, names):
        if data != None:
            table = pa.Table.from_pandas(data)
            buf = pa.BufferOutputStream()
            pq.write_table(table, buf)
            parquet_data = buf.getvalue().to_pybytes()
            blob_path_name = directory_name + &#34;/&#34; + blob_name
            if upload:
                self._container_client.upload_blob(
                    name=blob_path_name + &#34;.&#34; + format_type,
                    data=parquet_data,
                    overwrite=overwrite,
                )
            else:
                files.append(parquet_data)
        else:
            files_not_loaded.append(blob_name)
    if len(files_not_loaded) &gt; 0:
        return files_not_loaded

    if not upload:
        return files</code></pre>
</details>
</dd>
<dt id="pydbsmgr.utils.tools.tools.ControllerFeatures.write_pyarrow"><code class="name flex">
<span>def <span class="ident">write_pyarrow</span></span>(<span>self, directory_name: str, pytables: List[pyarrow.lib.Table], names: List[str], overwrite: bool = True) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Write pyarrow table as <code>parquet</code> format</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write_pyarrow(
    self,
    directory_name: str,
    pytables: List[Table],
    names: List[str],
    overwrite: bool = True,
) -&gt; None:
    &#34;&#34;&#34;Write pyarrow table as `parquet` format&#34;&#34;&#34;
    format_type = &#34;parquet&#34;
    files_not_loaded = []
    for table, blob_name in zip(pytables, names):
        if table != None:
            buf = pa.BufferOutputStream()
            pq.write_table(table, buf)
            parquet_data = buf.getvalue().to_pybytes()
            blob_path_name = directory_name + &#34;/&#34; + blob_name
            self._container_client.upload_blob(
                name=blob_path_name + &#34;.&#34; + format_type,
                data=parquet_data,
                overwrite=overwrite,
            )
        else:
            files_not_loaded.append(blob_name)
    if len(files_not_loaded) &gt; 0:
        return files_not_loaded</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pydbsmgr.utils.tools" href="index.html">pydbsmgr.utils.tools</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="pydbsmgr.utils.tools.tools.coerce_datetime" href="#pydbsmgr.utils.tools.tools.coerce_datetime">coerce_datetime</a></code></li>
<li><code><a title="pydbsmgr.utils.tools.tools.column_coincidence" href="#pydbsmgr.utils.tools.tools.column_coincidence">column_coincidence</a></code></li>
<li><code><a title="pydbsmgr.utils.tools.tools.create_directories_from_yaml" href="#pydbsmgr.utils.tools.tools.create_directories_from_yaml">create_directories_from_yaml</a></code></li>
<li><code><a title="pydbsmgr.utils.tools.tools.create_directory" href="#pydbsmgr.utils.tools.tools.create_directory">create_directory</a></code></li>
<li><code><a title="pydbsmgr.utils.tools.tools.disableprints" href="#pydbsmgr.utils.tools.tools.disableprints">disableprints</a></code></li>
<li><code><a title="pydbsmgr.utils.tools.tools.erase_files" href="#pydbsmgr.utils.tools.tools.erase_files">erase_files</a></code></li>
<li><code><a title="pydbsmgr.utils.tools.tools.generate_secure_password" href="#pydbsmgr.utils.tools.tools.generate_secure_password">generate_secure_password</a></code></li>
<li><code><a title="pydbsmgr.utils.tools.tools.get_extraction_date" href="#pydbsmgr.utils.tools.tools.get_extraction_date">get_extraction_date</a></code></li>
<li><code><a title="pydbsmgr.utils.tools.tools.merge_by_coincidence" href="#pydbsmgr.utils.tools.tools.merge_by_coincidence">merge_by_coincidence</a></code></li>
<li><code><a title="pydbsmgr.utils.tools.tools.most_repeated_item" href="#pydbsmgr.utils.tools.tools.most_repeated_item">most_repeated_item</a></code></li>
<li><code><a title="pydbsmgr.utils.tools.tools.terminate_process_holding_file" href="#pydbsmgr.utils.tools.tools.terminate_process_holding_file">terminate_process_holding_file</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="pydbsmgr.utils.tools.tools.ColumnsCheck" href="#pydbsmgr.utils.tools.tools.ColumnsCheck">ColumnsCheck</a></code></h4>
<ul class="">
<li><code><a title="pydbsmgr.utils.tools.tools.ColumnsCheck.get_frame" href="#pydbsmgr.utils.tools.tools.ColumnsCheck.get_frame">get_frame</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pydbsmgr.utils.tools.tools.ColumnsDtypes" href="#pydbsmgr.utils.tools.tools.ColumnsDtypes">ColumnsDtypes</a></code></h4>
<ul class="">
<li><code><a title="pydbsmgr.utils.tools.tools.ColumnsDtypes.correct" href="#pydbsmgr.utils.tools.tools.ColumnsDtypes.correct">correct</a></code></li>
<li><code><a title="pydbsmgr.utils.tools.tools.ColumnsDtypes.get_frame" href="#pydbsmgr.utils.tools.tools.ColumnsDtypes.get_frame">get_frame</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pydbsmgr.utils.tools.tools.ControllerFeatures" href="#pydbsmgr.utils.tools.tools.ControllerFeatures">ControllerFeatures</a></code></h4>
<ul class="">
<li><code><a title="pydbsmgr.utils.tools.tools.ControllerFeatures.write_parquet" href="#pydbsmgr.utils.tools.tools.ControllerFeatures.write_parquet">write_parquet</a></code></li>
<li><code><a title="pydbsmgr.utils.tools.tools.ControllerFeatures.write_pyarrow" href="#pydbsmgr.utils.tools.tools.ControllerFeatures.write_pyarrow">write_pyarrow</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>